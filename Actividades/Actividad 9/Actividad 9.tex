\documentclass[10pt]{article}
\usepackage[pdftex]{graphicx}
\graphicspath{{./images/}}
\usepackage{amsmath,amssymb}
\DeclareMathOperator*{\argmin}{\arg\!\min}
\usepackage{dirtytalk}
\usepackage{xcolor}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}
\usepackage[skip=10pt plus1pt, indent=40pt]{parskip}
\usepackage{../common_styles/csagh}


\begin{document}
\begin{opening}
  \title{Linear regression with Python}
  \author[Universidad Aut칩noma de Nuevo Le칩n, San Nicol치s de los Garza, aldo.hernandezt@uanl.edu.mx]{Aldo Hern치ndez}

  \keywords{...}
  \begin{abstract}
    %heh%
  \end{abstract}

  \keywords{linear regression, python, model, tendency, machine learning}
\end{opening}

\section{Introduction}
Linear regression is a fairly common algorithm in statistics and machine learning to make predictions and show tendencies given a certain dataset. This algorithm \textit{finds} a straight line---also known as "fitting" a line---that indicates a tendency and makes predictions from it \cite{aprendeML}. This model is widely used as an activation function on neural networks for some specific tasks but it is required to be combined with other non-linear functions in order to recognize more complex patterns. \par
The straight line is represented as
\begin{equation*}
    h_{w}(x) = w_{1}x + w_{0}
\end{equation*}
Where $\begin{bmatrix} w_{0} & w_{1} \end{bmatrix}$ is known as the weight vector and are the coefficients to be learned---or "found"---. The task of finding the $h_{w}$ that fits best this data is called \textbf{linear regression}. To do this, we simply have to find the values of the weight vector that minimize the empirical loss. It is traditional to use the squared loss function $L_{2}$ summed all over the training set \cite{ai} \par
\begin{equation*}
    Loss(h_{w}) = \sum_{j=1}^{N}L_{2}(y_{j}, h_{w}(x_{j})) = \sum_{j=1}^{N}(y_{j} - h_{w}(x_{j}))^{2} = \sum_{j=1}^{N}(y_{j} - (w_{1}x_{j} + w_{0}))^{2}
\end{equation*}

We would like to find $\textbf{w*} = \argmin_{w} Loss(h_{w})$. To minimize this function we have to partially differentiate with respect to each weight and set those equations to zero, since they will be at a minimum when their partial derivative is zero: \par
\begin{equation*}
    \begin{cases}
        \frac{\partial}{\partial w_{0}} \sum_{j=1}^{N}(y_{j} - (w_{1}x_{j} + w_{0}))^{2} = 0 \\
        \frac{\partial}{\partial w_{1}} \sum_{j=1}^{N}(y_{j} - (w_{1}x_{j} + w_{0}))^{2} = 0
    \end{cases} 
\end{equation*}

This equation system has a unique solution for the weight vector given by: \par
\begin{equation*}
    w_{1} = \frac{N(\Sigma x_{j}y_{j}) - (\Sigma x_{j})(\Sigma y_{j})}{N(\Sigma x_{j}^{2}) - (\Sigma x_{j})^2} \,;\, w_{0} = \frac{\Sigma y_{j} - w_{1}\Sigma x_{j}}{N}
\end{equation*}

Since in this way of learning we want to minimize a loss, it is useful to have a mental picture of what is going on in the \textbf{weight space}---the space defined by all possible settings of the weights. For univariate linear regression, this space is two-dimensional, hence we can graph the loss as a function of $w_{0}$ and $w_{1}$ in a 3D plot as seen in figure \ref{fig:loss_function}. \par

\newpage

\begin{figure}[h]
    \centering
    \includegraphics[width=60mm]{2025-03-26-09-24-32.png}
    \caption{Example loss function taken from \cite{ai}}
    \label{fig:loss_function}
\end{figure}

As we can see, this function is \textbf{convex}, and this is true for \textit{every} linear regression problem with an $L_{2}$ loss function, it also implies that there are no \textit{local minima}. \cite{ai} \par

When applying this concept in machine learning, it is important to note that we need a \textbf{learning rate} $\alpha$---also known as step size---that can be a fixed constant or decay over time, in purpose of minimizing the $Loss$ function. \par

For $N$ training samples, we want to minimize the sum of individual losses for each pair in the set, so we have
\begin{equation*}
    w_{0} \leftarrow w_{0} + \alpha\sum_{j}(y_{j} - h_{w}(x_{j})); \,\,\, w_{1} \leftarrow w_{1} + \alpha\sum_{j}(y_{j} - h_{w}(x_{j})) \times x_{j}
\end{equation*}

These updates constitute the \textbf{batch gradient descent} learning rule for univariate linear regression \cite{ai} that is a machine learning algorithm that uses the whole training dataset to optimize a training model. Convergence to a unique global minimum is guaranteed---as long as $\alpha$ is small enough---but may be vary slow since we have to cycle through all the training data for every step, and there may be a lot of steps \cite{ai}. This is essentially what linear regression models use to fit best the training dataset.

\newpage

\section{Methodology}
\subsection{Before typing code}
Before we start the activity, we must download the following \href{http://www.aprendemachinelearning.com/articulos_ml/}{.csv file} \cite{aprendeML} to use the same dataset. Also, we have to import the following Python packages:
\begin{itemize}
  \item Pandas
  \item Matplotlib
  \item Scikit-learn
\end{itemize}
After doing this, we are ready to import all of the needed packages to our Python main file: \par

\begin{figure}[h]
  \centering
  \includegraphics[width=90mm]{2025-03-26-14-16-48.png}
  \caption{Essential imports and styling.}
\end{figure}

As the reader can see, we also made some specific configuration for the program plots, such as size (in line 4) and style (in line 5); this may be changed according to the reader's preferences.

\subsection{Data processing}
Next, we will read the data from the .csv file using pandas to transform it into a dataframe. This is necessary since it will help us to easily remove all the columns that we will not use for the analysis, as shown in the figure: \par

\begin{figure}[h]
  \centering
  \includegraphics[width=90mm]{2025-03-26-14-19-30.png}
  \caption{Importing data and displaying useful information.}
\end{figure}

Using this information, we can see that the dataset contains 161 rows of data with 8 columns. Also, we can focus our interest in the columns "Word count" and "\# Shares", they will be our input and output for this model since we will try to predict the number of shares based on the word count of an article, as shown in table \ref{col_info}. \par

\begin{table}[!ht]
  \centering
  \caption{Column information}
  \label{col_info}
    \begin{tabular}{|c|c|c|}
      \hline
       & Word count & \# Shares \\
      \hline
      Count & 161 & 161 \\
      \hline
      Mean & 1808.26 & 27948.348 \\
      \hline
      Std & 1141.919 & 43408.007 \\
      \hline
    \end{tabular}
\end{table}

We can also create two histograms to look where is our dataframe more concentrated, as following \par

\begin{figure}[h]
  \centering
  \includegraphics[width=60mm]{word_count_hist.png}
  \includegraphics[width=60mm]{nshares_count_hist.png}
  \caption{Data concentration in both columns.}
  \label{fig:histograms}
\end{figure}

\subsection{Filtering data}
Now that we have all of our essential data, we need to filter it in order to remove random anomalies in the dataset. \par

Thanks to the histograms shown in Figure \ref{fig:histograms}, we can now filter the data to remove anomalies. To do so, we will simply remove all data with more than 3500 words and data with more than 80000 shares with the following code: \par

\begin{figure}[h]
  \centering
  \includegraphics[width=100mm]{2025-03-26-14-45-21.png}
  \caption{Filtering and coloring data.}
\end{figure}

We also colored data with red if they are over or on the word count mean, or blue otherwise.

\subsection{Creating the model}
After filtering our dataset, all we have left to do is create our linear regression model. For this, we will fit our model---or line---to the dataset and make predictions with the original data. Next, we will output some useful messages: \par

\begin{figure}[h]
  \centering
  \includegraphics[width=90mm]{2025-03-26-14-53-07.png}
  \caption{Training the linear regression model.}
  \label{fig:training}
\end{figure}

The piece of code shown in Figure \ref{fig:training} outputs the model's linear function $y \approx 5.7x + 11200.3$ with a very large mean squared error and small variance score, which are pretty bad results for said metrics.

\subsection{Making a graph}
Finally, we can plot our dataset in a 2D graph to visualize the straight line that represents our model: \par

\begin{figure}[h]
  \centering
  \includegraphics[width=80mm]{2025-03-26-15-07-35.png}
  \caption{Plotting a graph}
\end{figure}

\newpage

\begin{figure}[h]
  \centering
  \includegraphics[width=110mm]{2025-03-26-15-09-26.png}
  \caption{Word count vs Number of shares}
\end{figure}

\section{Results}
Even though our model is not trustworthy, we can still make some predictions with it. For example, let's suppose we have an article with 2000 words, according to our model, we would have around 22595 shares:
\begin{equation*}
  y \approx 5.7(2000) + 11200.3 \approx 22595
\end{equation*}

\section{Conclusions}
Guided by the used metrics, it turned out our model was not trustworthy given the dataset. Although it seems like a failure, the model helped to find a tendency: the more words an article has, the more shares it will have. Of course it is not as simple as that, but given these inputs, the model suggests that this is true. We could prove it by reducing dimensions using an algorithm like Principal Component Analysis, but that is out of the purpose of this article. \par
Therefore, we conclude that the model is not suitable for small and dispersed datasets since it will probably lead to an underfitted model that will make bad predictions, but it remains useful to find a tendency early on during a larger investigation.

\bibliographystyle{../common_styles/cs-agh}
\bibliography{act9_bibliography}

\end{document}