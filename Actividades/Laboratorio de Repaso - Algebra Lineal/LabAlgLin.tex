\documentclass[12pt, letterpaper]{article}
\usepackage[utf8]{inputenc}
\usepackage[letterpaper]{geometry}
\geometry{top=1.5cm, bottom=2.5cm, left=2.5cm, right=2.5cm}
\usepackage{amsmath}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage[skip=10pt plus1pt, indent=40pt]{parskip}
\pagestyle{fancy}
\fancypagestyle{plain}{}
\fancyhf{}
\lfoot[]{Inteligencia Artificial}
\rfoot[]{Enero - Junio 2025}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{1pt}
\usepackage{amsmath,amssymb}
\title{Laboratorio de Álgebra Lineal}
\author{Aldo Hernández}
\date{Febrero 2025}

\begin{document}
\maketitle

\section{Operaciones con matrices y determinantes}
\begin{enumerate}
    \item Consideremos la siguiente matriz F
    \begin{equation*}
        \begin{aligned}
            F
            & =
            \begin{pmatrix}
                1 & 2 & 3 &\bigm| & 1 & 0 & 0 \\
                0 & 1 & 4 &\bigm| & 0 & 1 & 0 \\
                5 & 6 & 0 &\bigm| & 0 & 0 & 1 
            \end{pmatrix}
            \sim
            \begin{pmatrix}
                1 & 2 & 3 &\bigm| & 1 & 0 & 0 \\
                5 & 6 & 0 &\bigm| & 0 & 0 & 1 \\
                0 & 1 & 4 &\bigm| & 0 & 1 & 0 
            \end{pmatrix} \\
            & \sim
            \begin{pmatrix}
                1 & 2 & 3 &\bigm| & 1 & 0 & 0 \\
                0 & -4 & -15 &\bigm| & -5 & 0 & 1 \\
                0 & 1 & 4 &\bigm| & 0 & 1 & 0 
            \end{pmatrix} 
            \sim
            \begin{pmatrix}
                1 & 2 & 3 &\bigm| & 1 & 0 & 0 \\
                0 & 1 & \frac{15}{4} &\bigm| & \frac{5}{4} & 0 & \frac{-1}{4} \\
                0 & 1 & 4 &\bigm| & 0 & 1 & 0 
            \end{pmatrix} \\
            & \sim
            \begin{pmatrix}
                1 & 0 & \frac{-9}{2} &\bigm| & \frac{-3}{2} & 0 & \frac{1}{2} \\
                0 & 1 & \frac{15}{4} &\bigm| & \frac{5}{4} & 0 & \frac{-1}{4} \\
                0 & 0 & \frac{1}{4} &\bigm| & \frac{-5}{4} & 1 & \frac{1}{4}
            \end{pmatrix}
            \sim
            \begin{pmatrix}
                1 & 0 & \frac{-9}{2} &\bigm| & \frac{-3}{2} & 0 & \frac{1}{2} \\
                0 & 1 & \frac{15}{4} &\bigm| & \frac{5}{4} & 0 & \frac{-1}{4} \\
                0 & 0 & 1 &\bigm| & -5 & 4 & 1
            \end{pmatrix} \\
            & \sim
            \begin{pmatrix}
                1 & 0 & 0 &\bigm| & -24 & 18 & 5 \\
                0 & 1 & 0 &\bigm| & 20 & -15 & -4 \\
                0 & 0 & 1 &\bigm| & -5 & 4 & 1
            \end{pmatrix} \\
            \Rightarrow F^{-1}
            & = 
            \begin{pmatrix}
                -24 & 18 & 5 \\
                20 & -15 & -4 \\
                -5 & 4 & 1
            \end{pmatrix}
        \end{aligned}
    \end{equation*}

    \newpage

    \item Sean $A$ y $B$ dos matrices de orden $n$ no necesariamente iguales, entonces si alguna de ellas no es invertible (no tiene inversa) podemos afirmar que su determinante es cero
    \begin{equation*}
        \Rightarrow \det AB = 0
    \end{equation*}
    Además se cumpliría la proposición
    \begin{equation*}
        \det A \cdot \det B = 0
    \end{equation*}
    En caso de que ambas matrices sean invertibles, entonces tienen inversa y pueden escribirse como productos de matrices elementales (matrices obtenidas al realizar operaciones elementales sobre una matriz identidad)
    \begin{equation*}
        A = E_{1} \cdot E_{2} \cdots E_{n}
    \end{equation*}
    \begin{equation*}
        B = F_{1} \cdot F_{2} \cdots F_{m}
    \end{equation*}
    \begin{equation*}
        \begin{aligned}
            \Rightarrow \det AB
            & = \det (E_{1} \cdot E_{2} \cdots E_{n} \cdots F_{1} \cdot F_{2} \cdots F_{m}) \\
            & = \det(E_{1}) \cdot \det(E_{2}) \cdots \det(E_{n}) \cdot \det(F_{1}) \cdot \det(F_{2}) \cdots \det(F_{m}) \\
            & = \det(E_{1} \cdot E_{2} \cdots E_{n}) \cdot \det(F_{1} \cdot F_{2} \cdots F_{m}) \\
            & = \det(A) \cdot \det(B) \blacksquare 
        \end{aligned}
    \end{equation*}

\end{enumerate}

\newpage

\section{Sistemas de ecuaciones lineales}
\begin{enumerate}
    \item Despejamos una variable de cada ecuación del sistema, de manera que tenemos
    
    \begin{equation*}
        x = \frac{7+y-z}{4}, y = \frac{1+2x+2z}{4}, z = \frac{5-x+y}{3}
    \end{equation*}
    
    Tomamos como solución inicial $x=y=z=0$
    \begin{itemize}
        \item Iteración 1
        \begin{equation*}
            \begin{aligned}
                x = 1.75 \\
                y = 1.13 \\
                z = 1.46
            \end{aligned}
        \end{equation*}
        \item Iteración 2
        \begin{equation*}
            \begin{aligned}
                x & = \frac{7+1.13-1.46}{4} = 1.67 \\
                y & = \frac{1+3.34+2.92}{4} = 1.82 \\
                z & = \frac{5-1.67+1.82}{3} = 1.72
            \end{aligned}
        \end{equation*}
        \item Iteración 3
        \begin{equation*}
            \begin{aligned}
                x & = \frac{7+1.82-1.72}{4} = 1.78 \\
                y & = \frac{1+3.56+3.44}{4} = 2 \\
                z & = \frac{5-1.78+2}{3} = 1.74
            \end{aligned}
        \end{equation*}
        \item Iteración 4
        \begin{equation*}
            \begin{aligned}
                x & = \frac{7+2-1.74}{4} = 1.82 \\
                y & = \frac{1+3.64+3.48}{4} = 2.03 \\
                z & = \frac{5-1.82+2.03}{3} = 1.74
            \end{aligned}
        \end{equation*}
        \item Iteración 5
        \begin{equation*}
            \begin{aligned}
                x & = \frac{7+2.03-1.74}{4} = 1.82 \\
                y & = \frac{1+3.64+3.48}{4} = 2.03 \\
                z & = \frac{5-1.82+2.03}{3} = 1.74
            \end{aligned}
        \end{equation*}
    \end{itemize}
    Observamos que llegamos a un error aproximado cercano al $0\%$, entonces concluimos como resultado lo siguiente
    \begin{equation*}
        \begin{aligned}
            x = 1.82 \\
            y = 2.03 \\
            z = 1.74
        \end{aligned}
    \end{equation*}
    \item Para el sistema homogéneo, podemos definir las siguientes matrices
    \begin{equation*}
        A =
        \begin{pmatrix}
            1 & 2 & 3 \\
            2 & 4 & 6 \\
            3 & 6 & 9
        \end{pmatrix},
        X =
        \begin{pmatrix}
            x \\
            y \\
            z
        \end{pmatrix},
        B =
        \begin{pmatrix}
            0 \\
            0 \\
            0
        \end{pmatrix}
    \end{equation*}
    De forma que $CX = B$, pero observamos que todos los renglones de C son múltiplos, es decir, son linealmente dependientes, por lo que al realizar operaciones elementales con $A$, eventualmente llegaremos al siguiente resultado
    \begin{equation*}
        \begin{aligned}
            A
            & \sim
            \begin{pmatrix}
                1 & 2 & 3 \\
                0 & 0 & 0 \\
                0 & 0 & 0
            \end{pmatrix} \\
            \Rightarrow x
            & = -2y-3z \\
            \Rightarrow X
            & =
            \begin{pmatrix}
                -2y-3z \\
                y \\
                z
            \end{pmatrix} \\
            & = y
            \begin{pmatrix}
                -2 \\
                1 \\
                0
            \end{pmatrix}
            + z
            \begin{pmatrix}
                -3 \\
                0 \\
                1
            \end{pmatrix}
        \end{aligned}
    \end{equation*}
    Entonces decimos que los vectores constantes
    $\begin{pmatrix}
        -2 \\
        1 \\
        0
    \end{pmatrix}$ y
    $\begin{pmatrix}
        -3 \\
        0 \\
        1
    \end{pmatrix}$
    son soluciones básicas del sistema de ecuaciones y que el sistema es compatible indeterminado con dos variables libres. Es por esto que hay soluciones infinitas dadas por el siguiente conjunto solución
    \begin{equation*}
        \left\{ 
            x = -2y-3z, y, z | x,y,z \in \mathbb{R} 
        \right\}
    \end{equation*}
\end{enumerate}

\newpage

\section{Espacios vectoriales y auto-valores/auto-vectores}
\begin{enumerate}
    \item Vemos que los vectores del subespacio dado son linealmente dependiemntes
    \begin{equation*}
        \left\{
            \begin{pmatrix}
                1 \\
                2 \\
                3
            \end{pmatrix},
            \begin{pmatrix}
                2 \\
                4 \\
                6
            \end{pmatrix},
            \begin{pmatrix}
                3 \\
                6 \\
                9
            \end{pmatrix}
        \right\}
        =
        \left\{
            \begin{pmatrix}
                1 \\
                2 \\
                3
            \end{pmatrix},
            2
            \begin{pmatrix}
                1 \\
                2 \\
                3
            \end{pmatrix},
            3
            \begin{pmatrix}
                1 \\
                2 \\
                3
            \end{pmatrix}
        \right\}
    \end{equation*}
    Entonces podemos concluir que una base para el subespacio vectorial es
    \begin{equation*}
        B =
        \left\{
            \begin{pmatrix}
                1 \\
                2 \\
                3
            \end{pmatrix}
        \right\}, \dim B = 1
    \end{equation*}
    \item Para hallar los eigenvalores de $G$ tenemos que igualar el siguiente determinante a cero
    \begin{equation*}
        \begin{aligned}
            |G - \lambda I|
            & = \begin{vmatrix}
                5-\lambda & -2 \\
                -2 & 5-\lambda
            \end{vmatrix} \\
            & = (5-\lambda)^2 - (-2)^2 \\
            \Rightarrow 0 & = (5-\lambda)^2 - (-2)^2 \\
            4 & = (5-\lambda)^2 \\
            \therefore \lambda_{1} & = 3 \\
            \therefore \lambda_{2} & = 7
        \end{aligned}
    \end{equation*}
    Para encontrar los eigenvectores, basta con sustituir los eigenvalores en $G-\lambda I$
    \begin{equation*}
        \begin{aligned}
            G - 3I =
            \begin{pmatrix}
                2 & -2 \\
                -2 & 2
            \end{pmatrix}
            & \sim
            \begin{pmatrix}
                2 & -2 \\
                0 & 0
            \end{pmatrix} \\
            \Rightarrow x_{1} & = x_{2}
        \end{aligned}
    \end{equation*}
    \begin{equation*}
        \begin{aligned}
            G - 7I =
            \begin{pmatrix}
                -2 & -2 \\
                -2 & -2
            \end{pmatrix}
            & \sim
            \begin{pmatrix}
                -2 & -2 \\
                0 & 0
            \end{pmatrix} \\
            \Rightarrow x_{1} & = -x_{2}
        \end{aligned}
    \end{equation*}
    \begin{equation*}
        \therefore 
        \begin{pmatrix}
            1 \\
            1
        \end{pmatrix}_{\lambda = 3}
        \land 
        \begin{pmatrix}
            -1 \\
            1
        \end{pmatrix}_{\lambda = 7},
    \end{equation*}
\end{enumerate}

\newpage

\section{Aplicaciones en IA: reducción de dimensionalidad}
\begin{enumerate}
    \item El Principal Component Analysis busca reducir el número de dimensiones en conjuntos de datos muy largos a componentes principales que mantengan la mayor cantidad de información original posible. Esto lo hace transformando las variables posiblemente correlacionadas en un conjunto más pequeño de variables no correlacionadas llamadas componentes principales. Básicamente, reduce el número de variables aleatorias con las que vamos a trabajar sin deshacernos de la información original más importante. \par
    Estos componentes son combinaciones lineales de las variables originales que tienen la mayor varianza a comparación de otras combinaciones lineales. \par
    Esta técnica estadística transforma el conjunto de datos original en un nuevo sistema de coordenadas estructurado por los componentes principales, los eigenvalores y eigenvectores de la matriz de covarianza permiten el análisis de estas transformaciones lineales. \par
    Por ejemplo, suponga que tiene un conjunto de datos y lo ha relacionado con múltiples características, resultando en una gráfica multidimensional de comparaciones de todas las características entre ellas mismas para reconocer patrones. Entonces, los eigenvectores otorgan la dirección de varianza en dicha gráfica y los eigenvalores denotan la importancia de esta dirección (a mayor valor, más crítico). \par
    Un caso práctico sería suponer que tenemos un conjunto de información muy grande acerca de algunos modelos de carros y nos interesa saber las relaciones que hay entre todas sus características, después de definirlas todas, estandarizamos todas nuestras variables aleatorias para que todas usen la misma escala. Después, creamos la matriz de covarianza entre todas las variables aleatorias para saber qué tan correlacionadas están entre ellas (en pares) y según los signos de los resultados podremos saber si las combinaciones están correlacionadas o no. Posteriormente obtenemos los eigenvectores (los componentes principales) y eigenvalores de la matriz, y ordenamos los vectores según sus valores propios. Seguido de ello seleccionamos los componentes principales con los que vamos a quedarnos, generalmente son aquellos con los eigenvalores más altos. Finalmente transformamos esta información en un nuevo sistema coordenado definido por los componentes principales, esto es proyectar los vectores de características creados por los eigenvectores de la matriz de covarianza en nuevos ejes definidos por los componentes principales, creando nueva información que captura la mayor parte de la original pero con menos dimensiones. 
    \newpage
    \item Buscamos las matrices $U_{2x2}$, $D_{2x2}$ y $V_{2x2}$ tales que se cumpla que
    \begin{equation*}
        \begin{pmatrix}
            3 & 1 \\
            2 & 2
        \end{pmatrix}
        = UDV^{T}
    \end{equation*}
    Definimos
    \begin{equation*}
        H = \begin{pmatrix}
            3 & 1 \\
            2 & 2
        \end{pmatrix},
        H^{T} = \begin{pmatrix}
            3 & 2 \\
            1 & 2
        \end{pmatrix}
    \end{equation*}
    Entonces
    \begin{equation*}
        \begin{pmatrix}
            3 & 1 \\
            2 & 2
        \end{pmatrix}
        \begin{pmatrix}
            3 & 2 \\
            1 & 2
        \end{pmatrix}
        = \begin{pmatrix}
            10 & 8 \\
            8 & 8
        \end{pmatrix},
        \begin{pmatrix}
            3 & 2 \\
            1 & 2
        \end{pmatrix}
        \begin{pmatrix}
            3 & 1 \\
            2 & 2
        \end{pmatrix}
        = \begin{pmatrix}
            13 & 7 \\
            7 & 5
        \end{pmatrix}
    \end{equation*}
    Encontramos los eigenvalores y eigenvectores de $AA^{T}$ y $A^{T}A$
    \begin{equation*}
        \begin{aligned}
            |AA^{T} - \lambda I| = 0
            & =
            \begin{vmatrix}
                10 - \lambda & 8 \\
                8 & 8 - \lambda
            \end{vmatrix} \\
            & = (10-\lambda)(8-\lambda) - 64 \\
            & = 80 - 18\lambda + \lambda ^{2} - 64 \\
            & = \lambda ^{2} - 18\lambda + 16 \\
            & \Rightarrow \lambda_{1} = 9+\sqrt{65}, \lambda_{2} = 9-\sqrt{65} \\
            & \Rightarrow v_{1} = \begin{pmatrix}
                1.133 \\
                1
            \end{pmatrix},
            v_{2} = \begin{pmatrix}
                -0.883 \\
                1
            \end{pmatrix}
        \end{aligned}
    \end{equation*}
    \begin{equation*} 
        \begin{aligned}
            |A^{T}A - \lambda I| = 0
            & =
            \begin{vmatrix}
                13 - \lambda & 7 \\
                7 & 5 - \lambda
            \end{vmatrix} \\
            & = (13-\lambda)(5-\lambda) - 49 \\
            & = 65 - 18\lambda + \lambda ^{2} - 49 \\
            & = \lambda ^{2} - 18\lambda + 16 \\
            & \Rightarrow \lambda_{1} = 9+\sqrt{65}, \lambda_{2} = 9-\sqrt{65} \\
            & \Rightarrow v_{1} = \begin{pmatrix}
                1.723 \\
                1
            \end{pmatrix},
            v_{2} = \begin{pmatrix}
                -0.580 \\
                1
            \end{pmatrix}
        \end{aligned}
    \end{equation*}
    Al normalizar cada vector característico tenemos que $L_{1} = 1.511$, $L_{2} = 1.334$, $L_{3} = 1.992$ y $L_{4} = 1.156$, entonces
    \begin{equation*}
        u_{1} = \begin{pmatrix}
            0.750 \\
            0.662
        \end{pmatrix},
        u_{2} = \begin{pmatrix}
            -0.662 \\
            0.750
        \end{pmatrix}
    \end{equation*}
    \begin{equation*}
        v_{1} = \begin{pmatrix}
            0.865 \\
            0.502
        \end{pmatrix},
        v_{2} = \begin{pmatrix}
            -0.502 \\
            0.865
        \end{pmatrix}
    \end{equation*}
    Por tanto tenemos que
    \begin{equation*}
        D = \begin{pmatrix}
            \sqrt{9+\sqrt{65}} & 0 \\
            0 & \sqrt{9-\sqrt{65}}
        \end{pmatrix}
        = \begin{pmatrix}
            4.130 & 0 \\
            0 & 0.968
        \end{pmatrix}
    \end{equation*}
    \begin{equation*}
        U = \begin{pmatrix}
            u_{1} & u_{2}
        \end{pmatrix}
        = \begin{pmatrix}
            0.750 & -0.662 \\
            0.662 & 0.750
        \end{pmatrix}
    \end{equation*}
    \begin{equation*}
        V = \begin{pmatrix}
            v_{1} & v_{2}
        \end{pmatrix}
        = \begin{pmatrix}
            0.865 & -0.502 \\
            0.502 & 0.865
        \end{pmatrix}
    \end{equation*}
    Finalmente concluimos la descomposición de la forma $H=UDV^{T}$
    \begin{equation*}
        \begin{pmatrix}
            3 & 1 \\
            2 & 2
        \end{pmatrix}
        =
        \begin{pmatrix}
            0.750 & -0.662 \\
            0.662 & 0.750
        \end{pmatrix}
        \begin{pmatrix}
            4.130 & 0 \\
            0 & 0.968
        \end{pmatrix}
        \begin{pmatrix}
            0.865 & 0.502 \\
            -0.502 & 0.865
        \end{pmatrix}
    \end{equation*}
    \item El álgebra lineal es fundamental para el aprendizaje profundo en redes neuronales por diversas razones, por ejemplo: 
    \begin{itemize}
        \item La forma de representación de los datos son vectores y matrices, al igual que las entradas y salidas de las redes neuronales.
        \item En cada capa de la red neuronal se realiza una multiplicación de matrices para transformar los datos antes de pasar a la siguiente capa.
        \item Las técnicas como el PCA o la SVD son útiles para reducir la dimensionalidad de los datos antes de alimentarlos a alguna red neuronal para mejorar la eficiencia del aprendizaje y la generalización de los modelos.
    \end{itemize}
    \item Los espacios vectoriales permiten representar, manipular y aprender a partir de datos en IA. A través de transformaciones lineales y reducción de dimensionalidad, los datos son proyectados en un espacio que facilita la captura de patrones y relaciones.
\end{enumerate}

\end{document}